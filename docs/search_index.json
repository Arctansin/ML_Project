[["index.html", "Machine Learning Project Chapter 1 Explonatory Analysis 1.1 Target Sampling 1.2 Split Variables 1.3 Missing Value 1.4 Separation Issue 1.5 Test data and the Final Data", " Machine Learning Project Mingming Li 2022-01-12 Chapter 1 Explonatory Analysis Training Data : train Testing Data : test 1.1 Target Sampling unique(train$INS) ## [1] 0 1 sum(train$INS==1)/length(train$INS) ## [1] 0.3434962 1.2 Split Variables Categorical variable level&lt;10 type=character col_unique&lt;-lapply(lapply(train,unique),length) catag_variable&lt;-names(col_unique[col_unique&lt;=10]) chara_type&lt;-lapply(train,typeof) chara_names&lt;-names(chara_type[chara_type==&quot;character&quot;]) catag_variable&lt;-unique(c(chara_names,catag_variable)) Continuous variable conti_variable&lt;-names(train) conti_variable&lt;-subset(conti_variable,!(conti_variable%in%catag_variable)) 1.3 Missing Value Finding the variable that has missing value Imputing the missing value in categorical variable as “Missing” Imputing the missing value in continuous variable as median, and adding related Flag #finding variables that have missing value na_check=colSums(is.na(train)) na_variable=na_check[na_check&gt;0] na_variable/nrow(train) ## ACCTAGE PHONE POS POSAMT INV INVBAL ## 0.06427310 0.12654503 0.12654503 0.12654503 0.12654503 0.12654503 ## CC CCBAL CCPURC INCOME LORES HMVAL ## 0.12654503 0.12654503 0.12654503 0.18092996 0.18092996 0.18092996 ## AGE CRSCORE ## 0.20035315 0.02295468 na_cata&lt;-names(na_variable)[names(na_variable) %in% catag_variable] na_conti&lt;-names(na_variable)[names(na_variable) %in% conti_variable] flag_matrix=matrix(rep(0,nrow(train)*length(na_conti)),nrow=nrow(train),ncol = length(na_conti)) colnames(flag_matrix)=c(paste(na_conti,&quot;_flag&quot;,sep=&quot;&quot;)) #impute the continuous variable for (i in seq(1:length(na_conti))){ mid_point=median(train[,na_conti[i]],na.rm = TRUE) flag_matrix[is.na(train[,na_conti[i]]),i]=1 train[,na_conti[i]]=replace_na(train[,na_conti[i]],mid_point) } #impute the categorical variable for (i in seq(1:length(na_cata))){ train[,na_cata[i]]=replace_na(train[,na_cata[i]],&quot;missing&quot;) } 1.4 Separation Issue Relationship between any catagorical variable and the target variable for (i in seq(1,length(catag_variable))){ print(catag_variable[i]) print(table(train$INS,train[,catag_variable[i]])) } ## [1] &quot;BRANCH&quot; ## ## B1 B10 B11 B12 B13 B14 B15 B16 B17 B18 B19 B2 B3 B4 B5 B6 B7 B8 ## 0 472 49 33 89 82 220 427 286 128 103 54 961 458 924 476 246 259 217 ## 1 259 33 26 51 62 51 141 94 83 55 24 454 269 557 267 146 134 150 ## ## B9 ## 0 93 ## 1 62 ## [1] &quot;DDA&quot; ## ## 0 1 ## 0 746 4831 ## 1 851 2067 ## [1] &quot;DIRDEP&quot; ## ## 0 1 ## 0 3793 1784 ## 1 2186 732 ## [1] &quot;NSF&quot; ## ## 0 1 ## 0 5015 562 ## 1 2747 171 ## [1] &quot;SAV&quot; ## ## 0 1 ## 0 3264 2313 ## 1 1270 1648 ## [1] &quot;ATM&quot; ## ## 0 1 ## 0 1956 3621 ## 1 1391 1527 ## [1] &quot;CD&quot; ## ## 0 1 ## 0 5170 407 ## 1 2298 620 ## [1] &quot;IRA&quot; ## ## 0 1 ## 0 5384 193 ## 1 2619 299 ## [1] &quot;INV&quot; ## ## 0 1 missing ## 0 4689 84 804 ## 1 2492 155 271 ## [1] &quot;MM&quot; ## ## 0 1 ## 0 5134 443 ## 1 2342 576 ## [1] &quot;MMCRED&quot; ## ## 0 1 2 3 5 ## 0 5409 130 33 4 1 ## 1 2713 153 47 5 0 ## [1] &quot;CC&quot; ## ## 0 1 missing ## 0 2729 2044 804 ## 1 1134 1513 271 ## [1] &quot;CCPURC&quot; ## ## 0 1 2 3 4 missing ## 0 4279 404 68 18 4 804 ## 1 2216 352 68 10 1 271 ## [1] &quot;SDB&quot; ## ## 0 1 ## 0 5053 524 ## 1 2514 404 ## [1] &quot;INAREA&quot; ## ## 0 1 ## 0 173 5404 ## 1 154 2764 ## [1] &quot;INS&quot; ## ## 0 1 ## 0 5577 0 ## 1 0 2918 Imputing the quasic-complete separation, and adding a corresponding flag train$MMCRED_sep_flag=0 train$MMCRED_sep_flag[train$MMCRED&gt;2]=1 train$MMCRED[train$MMCRED&gt;2]=&quot;3+&quot; new_train=cbind(train,flag_matrix) factorize the categorical variable for(i in c(catag_variable,&quot;MMCRED_sep_flag&quot;,colnames(flag_matrix))){ new_train[,i]=as.factor(new_train[,i]) } 1.5 Test data and the Final Data Do the same data posturing on the test data ### NA replace with &quot;missing&quot; or median value ## na_variable store all the vaiables with missing value na_check_train=colSums(is.na(train)) na_variable_train=na_check[na_check_train&gt;0] na_check=colSums(is.na(test)) na_variable=na_check[na_check&gt;0] na_variable %in% na_variable_train ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE na_variable/nrow(test) ## ACCTAGE PHONE POS POSAMT INV INVBAL ## 0.06214689 0.12146893 0.12146893 0.12146893 0.12146893 0.12146893 ## CC CCBAL CCPURC INCOME LORES HMVAL ## 0.12146893 0.12146893 0.12146893 0.18785311 0.18785311 0.18785311 ## AGE CRSCORE ## 0.20668550 0.02401130 na_cata&lt;-names(na_variable)[names(na_variable) %in% catag_variable] na_conti&lt;-names(na_variable)[names(na_variable) %in% conti_variable] flag_matrix=matrix(rep(0,nrow(test)*length(na_conti)),nrow=nrow(test),ncol = length(na_conti)) colnames(flag_matrix)=c(paste(na_conti,&quot;_flag&quot;,sep=&quot;&quot;)) #impute the continuous variable for (i in seq(1:length(na_conti))){ mid_point=median(test[,na_conti[i]],na.rm = TRUE) flag_matrix[is.na(test[,na_conti[i]]),i]=1 test[,na_conti[i]]=replace_na(test[,na_conti[i]],mid_point) } #impute the categorical variable for (i in seq(1:length(na_cata))){ test[,na_cata[i]]=replace_na(test[,na_cata[i]],&quot;missing&quot;) } test$MMCRED_sep_flag=0 test$MMCRED_sep_flag[test$MMCRED&gt;2]=1 test$MMCRED[test$MMCRED&gt;2]=&quot;3+&quot; new_test=cbind(test,flag_matrix) #factorize the categorical variable for(i in c(catag_variable,&quot;MMCRED_sep_flag&quot;,colnames(flag_matrix))){ new_test[,i]=as.factor(new_test[,i]) } Split the training into two pieces, train and val set.seed(123) index=sample(1:nrow(new_train),0.7*nrow(new_train)) train=new_train[index,] val=new_train[-(index),] test=new_test "],["model-building.html", "Chapter 2 Model Building 2.1 Earth algorithm 2.2 Random Forest model 2.3 XGBoost", " Chapter 2 Model Building 2.1 Earth algorithm mars2 &lt;- earth(INS ~ ., data = train,glm = list(family = binomial)) summary(mars2) ## Call: earth(formula=INS~., data=train, glm=list(family=binomial)) ## ## GLM coefficients ## 1 ## (Intercept) 12.4962153 ## DDA1 -1.1873152 ## IRA1 0.6075324 ## INV1 0.4968479 ## INVmissing -0.5275512 ## CC1 0.4248063 ## BRANCHB16 -0.5909910 ## h(5.6-ACCTAGE) 0.0786095 ## h(DDABAL-1304.97) -0.0005778 ## h(9011.31-DDABAL) -0.0006905 ## h(DDABAL-9011.31) 0.0005864 ## h(4-DEP) 0.1206177 ## h(CHECKS-1) -0.0402306 ## h(2-TELLER) -0.1595330 ## h(TELLER-2) 0.0728178 ## h(SAVBAL-1520.04) -0.0003573 ## h(6272.8-SAVBAL) -0.0005466 ## h(SAVBAL-6272.8) 0.0003550 ## h(12661.3-ATMAMT) -0.0000965 ## h(21300-CDBAL) -0.0000800 ## h(31366-MMBAL) -0.0000496 ## h(MMBAL-31366) -0.0000977 ## h(2654.41-CCBAL) 0.0001692 ## ## GLM (family binomial, link logit): ## nulldev df dev df devratio AIC iters converged ## 7680.57 5945 6177.36 5923 0.196 6223 4 1 ## ## Earth selected 23 of 29 terms, and 16 of 74 predictors ## Termination condition: RSq changed by less than 0.001 at 29 terms ## Importance: SAVBAL, MMBAL, CDBAL, DDA1, DDABAL, CC1, IRA1, CCBAL, ... ## Number of terms at each degree of interaction: 1 22 (additive model) ## Earth GCV 0.1755326 RSS 1027.979 GRSq 0.2260737 RSq 0.2374873 evimp(mars2) ## nsubsets gcv rss ## SAVBAL 22 100.0 100.0 ## MMBAL 20 68.4 69.9 ## CDBAL 20 63.8 65.6 ## DDA1 19 53.2 55.7 ## DDABAL 19 53.2 55.7 ## CC1 15 33.3 37.1 ## IRA1 15 29.8 34.2 ## CCBAL 14 30.6&gt; 34.5&gt; ## CHECKS 14 27.5 31.9 ## ATMAMT 13 25.9 30.3 ## TELLER 12 24.3 28.6 ## INVmissing 11 21.8 26.3 ## ACCTAGE 9 19.2 23.4 ## BRANCHB16 7 14.1 18.4 ## DEP 3 9.3 12.1 ## INV1 1 5.3 7.0 library(InformationValue) #ROC curve #validation ROC mars_val&lt;-predict(mars2,newdata=val,type=&quot;response&quot;) plotROC(val$INS,mars_val) library(caret) library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice library(Ckmeans.1d.dp) library(pdp) ## ## Attaching package: &#39;pdp&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## partial train&lt;-as.data.frame(train) val&lt;-as.data.frame(val) 2.2 Random Forest model build the model tune the tree I. ntree II.mtry III.remove the variable beneath “Random” at least plot the ROC for the last model set.seed(12345) rf.ins &lt;- randomForest(as.factor(INS) ~ ., data = train, ntree = 500, importance = TRUE,nodesize=10) # Plot the change in error across different number of trees plot(rf.ins, main = &quot;Number of Trees Compared to MSE&quot;) # tune II.mtry set.seed(12345) tuneRF(x = train[,!(names(train) %in% c(&quot;INS&quot;))], y = train[,&quot;INS&quot;], plot = TRUE, ntreeTry = 200, stepFactor = 0.5) ## mtry = 7 OOB error = 26.71% ## Searching left ... ## mtry = 14 OOB error = 26.89% ## -0.006926952 0.05 ## Searching right ... ## mtry = 3 OOB error = 27.14% ## -0.0163728 0.05 ## mtry OOBError ## 3.OOB 3 0.2714430 ## 7.OOB 7 0.2670703 ## 14.OOB 14 0.2689203 # final model set.seed(12345) rf.ins &lt;- randomForest(as.factor(INS) ~ ., data = train, ntree = 200, mtry = 7, importance = TRUE) # variable importance varImpPlot(rf.ins, sort = TRUE, n.var = 14, main = &quot;Order of Variables&quot;) importance(rf.ins, type = 1) ## MeanDecreaseAccuracy ## ACCTAGE 7.7249281 ## DDA 8.4683906 ## DDABAL 24.3820437 ## DEP 7.0575167 ## DEPAMT 13.6893071 ## CHECKS 9.4610135 ## DIRDEP 1.2711237 ## NSF -2.1841174 ## NSFAMT 2.2512549 ## PHONE 5.7620786 ## TELLER 4.1984060 ## SAV 5.4184715 ## SAVBAL 35.7106817 ## ATM 2.7541410 ## ATMAMT 13.1761269 ## POS 4.2470355 ## POSAMT 4.1150742 ## CD 11.5737432 ## CDBAL 21.2267748 ## IRA 8.1629640 ## IRABAL 11.7790066 ## INV 8.2431512 ## INVBAL 7.0545272 ## MM 10.0008370 ## MMBAL 13.0979211 ## MMCRED 4.6880704 ## CC 8.4260586 ## CCBAL 7.1536473 ## CCPURC 8.2301733 ## SDB 1.9180517 ## INCOME 5.7180897 ## LORES -0.3927448 ## HMVAL 5.2128266 ## AGE 0.2716806 ## CRSCORE 0.8604807 ## INAREA -1.1490497 ## BRANCH 12.3332145 ## MMCRED_sep_flag 1.0025094 ## ACCTAGE_flag -1.7002862 ## PHONE_flag 3.5256954 ## POS_flag 4.1391653 ## POSAMT_flag 4.0955869 ## INVBAL_flag 4.8061917 ## CCBAL_flag 4.4738315 ## INCOME_flag -0.7177039 ## LORES_flag 0.4378460 ## HMVAL_flag -0.4254986 ## AGE_flag 0.3924822 ## CRSCORE_flag -0.6571605 # ROC rf_val&lt;-predict(rf.ins,newdata=val[,!(names(val) %in% c(&quot;INS&quot;))],type=&quot;prob&quot;) rf_val&lt;-as.data.frame(rf_val) print(plotROC(val$INS,rf_val[,2])) ## NULL # variable importance - adding a random variable train$random &lt;- rnorm(nrow(train)) set.seed(12345) rf.ins &lt;- randomForest(as.factor(INS) ~ ., data = train, ntree = 200, mtry = 7, importance = TRUE) varImpPlot(rf.ins, sort = TRUE, n.var = 30, main = &quot;Look for Variables Below Random Variable&quot;) importance(rf.ins) ## 0 1 MeanDecreaseAccuracy ## ACCTAGE 5.959552843 0.7310872 5.58836331 ## DDA -0.740745155 10.5384807 7.64998480 ## DDABAL 12.891464937 16.4939155 24.06650773 ## DEP 3.807834950 4.2207589 8.47327583 ## DEPAMT 9.661971031 0.9093551 12.57626399 ## CHECKS 11.448351916 -2.2836202 9.95483095 ## DIRDEP 2.472452700 1.2156751 2.90244886 ## NSF -2.686683401 4.9656198 0.15833651 ## NSFAMT -2.276389261 5.2106125 0.50962091 ## PHONE 7.496190140 1.9772249 7.85743804 ## TELLER 0.722661622 4.2718063 4.05156099 ## SAV -7.676875632 10.4214169 4.24151497 ## SAVBAL 14.186015078 35.4039121 29.28259923 ## ATM 4.664672735 -0.5004376 4.84392091 ## ATMAMT 9.692419242 0.5067671 9.49196511 ## POS 4.845696039 -0.7532155 4.04628721 ## POSAMT 3.992384127 -0.7684375 3.57282918 ## CD 7.777093372 10.2322245 13.71947981 ## CDBAL 13.892514608 14.3991730 19.87310370 ## IRA 8.098771085 2.7911502 8.74244874 ## IRABAL 11.658143882 3.2350990 12.16102132 ## INV 11.114913378 -6.1620820 9.30821808 ## INVBAL 7.450235241 0.5264275 7.24009121 ## MM 7.043810872 6.0181927 9.33051376 ## MMBAL 7.977707865 7.0031193 11.57894921 ## MMCRED 4.136079956 -1.0971035 2.78788472 ## CC 10.135272955 -2.7068093 9.85892763 ## CCBAL 8.239975589 -2.3655755 6.28238761 ## CCPURC 8.492930669 -0.2887163 8.99725481 ## SDB 1.997646509 1.2787764 2.31286381 ## INCOME 6.579894285 -0.3344054 5.91966254 ## LORES -0.025270567 -1.5058702 -1.01306509 ## HMVAL 6.510446158 -1.0035653 5.31588450 ## AGE 1.092310954 -1.1178351 0.12556353 ## CRSCORE 1.071824857 -1.4567061 -0.02845033 ## INAREA -0.157233444 -0.2211701 -0.34199664 ## BRANCH 15.953962299 -5.4546438 11.45317043 ## MMCRED_sep_flag 0.000000000 -1.0025094 -1.00250941 ## ACCTAGE_flag -0.320980633 2.2409982 0.86597462 ## PHONE_flag 4.629027838 -3.3817170 3.82088950 ## POS_flag 4.935343754 -2.5006179 4.47781508 ## POSAMT_flag 4.616333714 -3.4685790 3.84546565 ## INVBAL_flag 5.969841920 -2.9419033 5.43347584 ## CCBAL_flag 3.356682393 -3.0709093 2.55547601 ## INCOME_flag -1.053180917 -0.7026002 -1.40434029 ## LORES_flag -0.514518898 -1.0867531 -1.15553667 ## HMVAL_flag -0.050259720 1.1404694 0.66979028 ## AGE_flag 2.523010145 -0.3022451 2.07642819 ## CRSCORE_flag 0.006144934 -1.2730337 -0.72272520 ## random -1.672679280 -2.4969155 -3.07190043 ## MeanDecreaseGini ## ACCTAGE 137.8936321 ## DDA 31.7584274 ## DDABAL 208.1301747 ## DEP 54.2958720 ## DEPAMT 118.5824139 ## CHECKS 73.0400294 ## DIRDEP 17.0313889 ## NSF 7.6533416 ## NSFAMT 16.0586464 ## PHONE 19.3921512 ## TELLER 54.2774540 ## SAV 30.5474732 ## SAVBAL 267.4020046 ## ATM 16.6119460 ## ATMAMT 95.4226612 ## POS 25.3657478 ## POSAMT 31.2171439 ## CD 39.8915494 ## CDBAL 81.3316944 ## IRA 16.7096137 ## IRABAL 29.3609401 ## INV 15.2199339 ## INVBAL 8.0735563 ## MM 28.2696690 ## MMBAL 57.8211458 ## MMCRED 10.4789471 ## CC 32.3188487 ## CCBAL 71.4471566 ## CCPURC 28.4510415 ## SDB 17.4869900 ## INCOME 109.3828512 ## LORES 90.3838928 ## HMVAL 107.2512708 ## AGE 105.1187741 ## CRSCORE 132.0633896 ## INAREA 7.4895176 ## BRANCH 228.7537541 ## MMCRED_sep_flag 0.2687878 ## ACCTAGE_flag 10.9661076 ## PHONE_flag 3.8791871 ## POS_flag 4.1282022 ## POSAMT_flag 4.2728668 ## INVBAL_flag 4.2842559 ## CCBAL_flag 3.9186899 ## INCOME_flag 9.5754510 ## LORES_flag 9.3170606 ## HMVAL_flag 9.4515647 ## AGE_flag 10.6798981 ## CRSCORE_flag 5.5682450 ## random 144.4528166 2.3 XGBoost build the model tune the model at least plot the ROC for the best model # Prepare data for XGBoost function - similar to what we did for glmnet train=train[,1:49] ## remove random from ealier train_x &lt;- model.matrix(INS ~ ., data = train) train_y &lt;- train$INS # Build XGBoost model param &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) set.seed(12345) xgb.ins &lt;- xgboost(param,data = train_x, label = train_y, subsample = 0.5, nrounds = 100) ## [1] train-rmse:0.743939 ## [2] train-rmse:0.597326 ## [3] train-rmse:0.508699 ## [4] train-rmse:0.456657 ## [5] train-rmse:0.424565 ## [6] train-rmse:0.406474 ## [7] train-rmse:0.395218 ## [8] train-rmse:0.387726 ## [9] train-rmse:0.382124 ## [10] train-rmse:0.377626 ## [11] train-rmse:0.373750 ## [12] train-rmse:0.370452 ## [13] train-rmse:0.368416 ## [14] train-rmse:0.365124 ## [15] train-rmse:0.361817 ## [16] train-rmse:0.358936 ## [17] train-rmse:0.356425 ## [18] train-rmse:0.354942 ## [19] train-rmse:0.353357 ## [20] train-rmse:0.351120 ## [21] train-rmse:0.347475 ## [22] train-rmse:0.346513 ## [23] train-rmse:0.343570 ## [24] train-rmse:0.341695 ## [25] train-rmse:0.340232 ## [26] train-rmse:0.339193 ## [27] train-rmse:0.337053 ## [28] train-rmse:0.335205 ## [29] train-rmse:0.332573 ## [30] train-rmse:0.331137 ## [31] train-rmse:0.329355 ## [32] train-rmse:0.326814 ## [33] train-rmse:0.325557 ## [34] train-rmse:0.324208 ## [35] train-rmse:0.323797 ## [36] train-rmse:0.322547 ## [37] train-rmse:0.320210 ## [38] train-rmse:0.317942 ## [39] train-rmse:0.316545 ## [40] train-rmse:0.314690 ## [41] train-rmse:0.312841 ## [42] train-rmse:0.310454 ## [43] train-rmse:0.308170 ## [44] train-rmse:0.306803 ## [45] train-rmse:0.303870 ## [46] train-rmse:0.301983 ## [47] train-rmse:0.300419 ## [48] train-rmse:0.298078 ## [49] train-rmse:0.295030 ## [50] train-rmse:0.293430 ## [51] train-rmse:0.292413 ## [52] train-rmse:0.290634 ## [53] train-rmse:0.288425 ## [54] train-rmse:0.286742 ## [55] train-rmse:0.284857 ## [56] train-rmse:0.282265 ## [57] train-rmse:0.280089 ## [58] train-rmse:0.278802 ## [59] train-rmse:0.277760 ## [60] train-rmse:0.276745 ## [61] train-rmse:0.275726 ## [62] train-rmse:0.274406 ## [63] train-rmse:0.273184 ## [64] train-rmse:0.272104 ## [65] train-rmse:0.270476 ## [66] train-rmse:0.268728 ## [67] train-rmse:0.268088 ## [68] train-rmse:0.266523 ## [69] train-rmse:0.265185 ## [70] train-rmse:0.263131 ## [71] train-rmse:0.260770 ## [72] train-rmse:0.258493 ## [73] train-rmse:0.256999 ## [74] train-rmse:0.256086 ## [75] train-rmse:0.254243 ## [76] train-rmse:0.252435 ## [77] train-rmse:0.251422 ## [78] train-rmse:0.249583 ## [79] train-rmse:0.247409 ## [80] train-rmse:0.245927 ## [81] train-rmse:0.244472 ## [82] train-rmse:0.242767 ## [83] train-rmse:0.240665 ## [84] train-rmse:0.239206 ## [85] train-rmse:0.238278 ## [86] train-rmse:0.237787 ## [87] train-rmse:0.236684 ## [88] train-rmse:0.234600 ## [89] train-rmse:0.233602 ## [90] train-rmse:0.232238 ## [91] train-rmse:0.230865 ## [92] train-rmse:0.229550 ## [93] train-rmse:0.228411 ## [94] train-rmse:0.226925 ## [95] train-rmse:0.225454 ## [96] train-rmse:0.224118 ## [97] train-rmse:0.223376 ## [98] train-rmse:0.222220 ## [99] train-rmse:0.220692 ## [100] train-rmse:0.219115 #Tuning an XGBoost nrounds parameter - 24 was lowest! xgbcv.ins &lt;- xgb.cv(data = train_x, label = train_y, subsample = 0.5, nrounds = 100, nfold = 10) ## [1] train-rmse:0.744934+0.002694 test-rmse:0.748296+0.017834 ## [2] train-rmse:0.598602+0.003129 test-rmse:0.607432+0.016847 ## [3] train-rmse:0.509293+0.002651 test-rmse:0.525793+0.016050 ## [4] train-rmse:0.455353+0.001781 test-rmse:0.478327+0.015103 ## [5] train-rmse:0.424221+0.001370 test-rmse:0.452760+0.013533 ## [6] train-rmse:0.405262+0.001033 test-rmse:0.440055+0.012884 ## [7] train-rmse:0.393888+0.001162 test-rmse:0.434528+0.011799 ## [8] train-rmse:0.386223+0.001194 test-rmse:0.431434+0.011486 ## [9] train-rmse:0.380438+0.001791 test-rmse:0.430704+0.010649 ## [10] train-rmse:0.375871+0.001613 test-rmse:0.429892+0.010374 ## [11] train-rmse:0.371946+0.002108 test-rmse:0.430141+0.010113 ## [12] train-rmse:0.368583+0.002054 test-rmse:0.430271+0.010295 ## [13] train-rmse:0.365393+0.002396 test-rmse:0.430797+0.010576 ## [14] train-rmse:0.362445+0.002982 test-rmse:0.431887+0.010798 ## [15] train-rmse:0.359860+0.002716 test-rmse:0.432171+0.009885 ## [16] train-rmse:0.357109+0.002719 test-rmse:0.432944+0.010431 ## [17] train-rmse:0.354258+0.002600 test-rmse:0.432835+0.010273 ## [18] train-rmse:0.351881+0.002477 test-rmse:0.434232+0.009653 ## [19] train-rmse:0.348810+0.002711 test-rmse:0.435536+0.009840 ## [20] train-rmse:0.346248+0.001954 test-rmse:0.436197+0.010150 ## [21] train-rmse:0.343916+0.001949 test-rmse:0.436938+0.009819 ## [22] train-rmse:0.341219+0.002255 test-rmse:0.437682+0.009997 ## [23] train-rmse:0.339197+0.002402 test-rmse:0.438820+0.011045 ## [24] train-rmse:0.336588+0.002376 test-rmse:0.439122+0.011106 ## [25] train-rmse:0.334738+0.002547 test-rmse:0.439608+0.010982 ## [26] train-rmse:0.332800+0.002473 test-rmse:0.440636+0.011021 ## [27] train-rmse:0.330114+0.002415 test-rmse:0.440873+0.010766 ## [28] train-rmse:0.327518+0.002073 test-rmse:0.442193+0.011013 ## [29] train-rmse:0.325362+0.002078 test-rmse:0.442777+0.011209 ## [30] train-rmse:0.322897+0.002115 test-rmse:0.443360+0.010526 ## [31] train-rmse:0.320307+0.001989 test-rmse:0.443919+0.010525 ## [32] train-rmse:0.318011+0.001875 test-rmse:0.444443+0.010604 ## [33] train-rmse:0.315834+0.001834 test-rmse:0.444851+0.010969 ## [34] train-rmse:0.313586+0.002074 test-rmse:0.445440+0.011017 ## [35] train-rmse:0.311748+0.002308 test-rmse:0.445879+0.011036 ## [36] train-rmse:0.309563+0.002157 test-rmse:0.446626+0.010704 ## [37] train-rmse:0.307324+0.001901 test-rmse:0.447461+0.010439 ## [38] train-rmse:0.305169+0.001832 test-rmse:0.447540+0.010176 ## [39] train-rmse:0.302945+0.002218 test-rmse:0.447917+0.010818 ## [40] train-rmse:0.300907+0.002237 test-rmse:0.448380+0.010690 ## [41] train-rmse:0.298999+0.002075 test-rmse:0.448933+0.010559 ## [42] train-rmse:0.297194+0.002137 test-rmse:0.449324+0.010665 ## [43] train-rmse:0.295285+0.002237 test-rmse:0.449936+0.010703 ## [44] train-rmse:0.293213+0.002161 test-rmse:0.451241+0.010437 ## [45] train-rmse:0.291370+0.002012 test-rmse:0.451810+0.010270 ## [46] train-rmse:0.289805+0.002031 test-rmse:0.452307+0.009959 ## [47] train-rmse:0.287932+0.002350 test-rmse:0.453272+0.010107 ## [48] train-rmse:0.285924+0.002659 test-rmse:0.453755+0.010086 ## [49] train-rmse:0.284012+0.002695 test-rmse:0.453609+0.010433 ## [50] train-rmse:0.281964+0.002601 test-rmse:0.454477+0.011072 ## [51] train-rmse:0.280050+0.002969 test-rmse:0.454666+0.010968 ## [52] train-rmse:0.278077+0.002747 test-rmse:0.454891+0.010974 ## [53] train-rmse:0.276149+0.002695 test-rmse:0.455659+0.010872 ## [54] train-rmse:0.274494+0.002682 test-rmse:0.456310+0.010940 ## [55] train-rmse:0.272774+0.002648 test-rmse:0.457004+0.010993 ## [56] train-rmse:0.270763+0.002649 test-rmse:0.457800+0.010763 ## [57] train-rmse:0.269015+0.002774 test-rmse:0.458884+0.010671 ## [58] train-rmse:0.267287+0.002955 test-rmse:0.459142+0.010634 ## [59] train-rmse:0.265344+0.003282 test-rmse:0.459592+0.010694 ## [60] train-rmse:0.263574+0.003148 test-rmse:0.460236+0.010500 ## [61] train-rmse:0.261858+0.003478 test-rmse:0.460781+0.010569 ## [62] train-rmse:0.260213+0.003520 test-rmse:0.461309+0.010512 ## [63] train-rmse:0.258433+0.003938 test-rmse:0.461528+0.010584 ## [64] train-rmse:0.256991+0.004182 test-rmse:0.461537+0.010370 ## [65] train-rmse:0.255620+0.004223 test-rmse:0.461765+0.010399 ## [66] train-rmse:0.254136+0.003920 test-rmse:0.461700+0.010557 ## [67] train-rmse:0.252469+0.003858 test-rmse:0.462018+0.010307 ## [68] train-rmse:0.251162+0.003761 test-rmse:0.462590+0.010685 ## [69] train-rmse:0.249628+0.003841 test-rmse:0.462921+0.010585 ## [70] train-rmse:0.248038+0.003601 test-rmse:0.463412+0.010133 ## [71] train-rmse:0.246457+0.003607 test-rmse:0.463695+0.009794 ## [72] train-rmse:0.244870+0.003664 test-rmse:0.463934+0.009922 ## [73] train-rmse:0.243200+0.003442 test-rmse:0.464193+0.009375 ## [74] train-rmse:0.241697+0.003584 test-rmse:0.464349+0.009213 ## [75] train-rmse:0.239959+0.003478 test-rmse:0.464921+0.009183 ## [76] train-rmse:0.238490+0.003271 test-rmse:0.465385+0.009101 ## [77] train-rmse:0.236859+0.003386 test-rmse:0.465677+0.009422 ## [78] train-rmse:0.235136+0.003289 test-rmse:0.466270+0.009562 ## [79] train-rmse:0.233437+0.003302 test-rmse:0.466269+0.009619 ## [80] train-rmse:0.231662+0.003271 test-rmse:0.466835+0.009715 ## [81] train-rmse:0.230068+0.003422 test-rmse:0.467339+0.009397 ## [82] train-rmse:0.228556+0.003378 test-rmse:0.467919+0.009576 ## [83] train-rmse:0.227307+0.003398 test-rmse:0.467959+0.009419 ## [84] train-rmse:0.225958+0.003437 test-rmse:0.468129+0.009857 ## [85] train-rmse:0.224424+0.003473 test-rmse:0.468403+0.009589 ## [86] train-rmse:0.223147+0.003625 test-rmse:0.468205+0.010063 ## [87] train-rmse:0.221580+0.003601 test-rmse:0.469016+0.009900 ## [88] train-rmse:0.220001+0.003785 test-rmse:0.469399+0.010163 ## [89] train-rmse:0.218758+0.003881 test-rmse:0.469576+0.010180 ## [90] train-rmse:0.217445+0.003668 test-rmse:0.469820+0.009891 ## [91] train-rmse:0.215735+0.003452 test-rmse:0.470071+0.010319 ## [92] train-rmse:0.214368+0.003399 test-rmse:0.469826+0.010550 ## [93] train-rmse:0.212918+0.003232 test-rmse:0.469580+0.010799 ## [94] train-rmse:0.211684+0.003347 test-rmse:0.469377+0.010903 ## [95] train-rmse:0.210598+0.003532 test-rmse:0.469433+0.010887 ## [96] train-rmse:0.209357+0.003591 test-rmse:0.469440+0.011157 ## [97] train-rmse:0.207888+0.003523 test-rmse:0.469756+0.011293 ## [98] train-rmse:0.206655+0.003359 test-rmse:0.470104+0.011187 ## [99] train-rmse:0.205494+0.003241 test-rmse:0.470012+0.010899 ## [100] train-rmse:0.204083+0.003078 test-rmse:0.470495+0.011324 best_nrounds=which.min(xgbcv.ins$evaluation_log$test_rmse_mean) # Tuning through caret tune_grid &lt;- expand.grid( nrounds = best_nrounds, eta = c(0.1, 0.15, 0.2, 0.25, 0.3), max_depth = c(1:10), gamma = c(0), colsample_bytree = 1, min_child_weight = 1, subsample = c(0.25, 0.5, 0.75, 1) ) # train_x&lt;-as.data.frame(train_x) # train_y&lt;-as.data.frame(train_y) set.seed(12345) xgb.ames.caret &lt;- train(x = train_x, y = train_y, method = &quot;xgbTree&quot;, tuneGrid = tune_grid, trControl = trainControl(method = &#39;cv&#39;, # Using 10-fold cross-validation number = 10)) plot(xgb.ames.caret) ###&gt;&gt;&gt;&gt; best tune_grid # nrounds = best_nrounds, # eta = c(0.3), # max_depth = c(5), # subsample = c(0.75) #best_model best_xgb.ins &lt;- xgboost(param,data = train_x, label = train_y, subsample = 0.75, nrounds = 12, eta = 0.3, max_depth = 5) ## [1] train-rmse:0.744197 ## [2] train-rmse:0.599352 ## [3] train-rmse:0.510680 ## [4] train-rmse:0.459831 ## [5] train-rmse:0.430101 ## [6] train-rmse:0.412594 ## [7] train-rmse:0.402094 ## [8] train-rmse:0.395384 ## [9] train-rmse:0.390647 ## [10] train-rmse:0.387338 ## [11] train-rmse:0.384222 ## [12] train-rmse:0.381584 #training xgb_train&lt;-predict(best_xgb.ins,train_x) plotROC(train$INS,xgb_train) ###validation Varaible selected and remove random val=val[,1:49] ## remove random from ealier val_x &lt;- model.matrix(INS ~ ., data = val) val_y &lt;- val$INS xgb_val&lt;-predict(best_xgb.ins,val_x) plotROC(val$INS,xgb_val) # Variable importance variable_importance_xgb &lt;- function(){ param &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) set.seed(12345) best_xgb.ins &lt;- xgboost(param,data = train_x, label = train_y, subsample = 0.75, nrounds = best_nrounds, eta = 0.3, max_depth = 5) print(xgb.importance(feature_names = colnames(train_x), model = best_xgb.ins)) print(xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = best_xgb.ins))) } variable_importance_xgb() ## [1] train-rmse:0.744638 ## [2] train-rmse:0.596814 ## [3] train-rmse:0.508874 ## [4] train-rmse:0.456191 ## [5] train-rmse:0.427241 ## [6] train-rmse:0.409849 ## [7] train-rmse:0.399919 ## [8] train-rmse:0.393070 ## [9] train-rmse:0.389109 ## [10] train-rmse:0.384957 ## Feature Gain Cover Frequency ## 1: SAVBAL 0.2906748990 1.622810e-01 0.084249084 ## 2: DDABAL 0.1157978583 1.355142e-01 0.091575092 ## 3: MMBAL 0.0939542359 8.806632e-02 0.029304029 ## 4: CDBAL 0.0919033957 1.080036e-01 0.062271062 ## 5: DDA1 0.0683990730 5.800583e-02 0.021978022 ## 6: ACCTAGE 0.0315231260 3.984315e-02 0.080586081 ## 7: HMVAL 0.0264171842 3.481067e-02 0.051282051 ## 8: CRSCORE 0.0239983536 1.174994e-02 0.051282051 ## 9: INCOME 0.0211622127 1.520502e-02 0.051282051 ## 10: IRABAL 0.0209293683 3.922474e-02 0.032967033 ## 11: CHECKS 0.0207376795 1.969079e-02 0.029304029 ## 12: AGE 0.0192146542 9.679588e-03 0.040293040 ## 13: CCBAL 0.0187976560 2.004481e-02 0.040293040 ## 14: ATMAMT 0.0172953014 3.083576e-02 0.029304029 ## 15: DEPAMT 0.0161197345 8.043917e-03 0.036630037 ## 16: TELLER 0.0153666016 2.327582e-02 0.036630037 ## 17: DEP 0.0132051750 2.024647e-02 0.036630037 ## 18: INVmissing 0.0110150015 1.660318e-02 0.010989011 ## 19: LORES 0.0081019090 1.689447e-03 0.025641026 ## 20: IRA1 0.0070951619 9.831952e-03 0.010989011 ## 21: BRANCHB16 0.0068372296 5.287923e-03 0.010989011 ## 22: INV1 0.0061921058 3.803719e-02 0.007326007 ## 23: CCPURC1 0.0057192063 1.952050e-02 0.010989011 ## 24: CC1 0.0056463734 2.014340e-02 0.010989011 ## 25: POSAMT 0.0050380645 8.335201e-04 0.010989011 ## 26: NSFAMT 0.0039195909 4.611248e-03 0.010989011 ## 27: BRANCHB14 0.0036407623 7.456868e-03 0.007326007 ## 28: MM1 0.0031728391 6.762268e-03 0.003663004 ## 29: BRANCHB8 0.0029287621 9.769214e-04 0.007326007 ## 30: POS 0.0027930609 5.780865e-03 0.007326007 ## 31: DIRDEP1 0.0026224264 2.072597e-02 0.007326007 ## 32: BRANCHB13 0.0024863095 9.831952e-03 0.003663004 ## 33: INVBAL 0.0018822434 3.629845e-04 0.003663004 ## 34: BRANCHB15 0.0017552331 2.007618e-03 0.003663004 ## 35: SDB1 0.0017274601 7.663007e-04 0.003663004 ## 36: PHONE 0.0017182364 3.181716e-04 0.003663004 ## 37: BRANCHB12 0.0017087346 7.259691e-04 0.003663004 ## 38: BRANCHB10 0.0016294698 1.232355e-03 0.007326007 ## 39: BRANCHB6 0.0015629375 4.122787e-04 0.003663004 ## 40: BRANCHB3 0.0013365828 8.872955e-04 0.003663004 ## 41: BRANCHB5 0.0012841840 1.792516e-04 0.003663004 ## 42: BRANCHB9 0.0011116029 1.971768e-04 0.003663004 ## 43: INCOME_flag1 0.0008186868 7.170065e-05 0.003663004 ## 44: BRANCHB18 0.0007593168 2.240645e-04 0.003663004 ## Feature Gain Cover Frequency # Include a random variable to determine variable selection train$random &lt;- rnorm(nrow(train)) train_x &lt;- model.matrix(INS ~ ., data = train)[, -1] train_y &lt;- train$INS variable_importance_xgb() ## [1] train-rmse:0.744571 ## [2] train-rmse:0.596738 ## [3] train-rmse:0.508788 ## [4] train-rmse:0.455890 ## [5] train-rmse:0.426759 ## [6] train-rmse:0.409214 ## [7] train-rmse:0.399463 ## [8] train-rmse:0.392477 ## [9] train-rmse:0.388069 ## [10] train-rmse:0.384741 ## Feature Gain Cover Frequency ## 1: SAVBAL 0.2871346012 1.757926e-01 0.075812274 ## 2: DDABAL 0.1077247344 1.299330e-01 0.068592058 ## 3: MMBAL 0.0946192392 9.677419e-02 0.032490975 ## 4: CDBAL 0.0922039755 1.250482e-01 0.057761733 ## 5: DDA1 0.0652364627 5.213366e-02 0.014440433 ## 6: ACCTAGE 0.0329941238 3.404620e-02 0.079422383 ## 7: CRSCORE 0.0269250302 1.428264e-02 0.061371841 ## 8: random 0.0240615085 2.901792e-02 0.061371841 ## 9: CHECKS 0.0232215400 2.433472e-02 0.028880866 ## 10: HMVAL 0.0222971233 1.131587e-02 0.050541516 ## 11: CCBAL 0.0189343142 1.579740e-02 0.039711191 ## 12: INCOME 0.0188717463 1.236006e-02 0.043321300 ## 13: AGE 0.0183586305 4.145417e-03 0.036101083 ## 14: DEPAMT 0.0182947896 1.049575e-02 0.039711191 ## 15: IRABAL 0.0180152322 3.340982e-02 0.025270758 ## 16: ATMAMT 0.0174945311 3.257177e-02 0.036101083 ## 17: TELLER 0.0122687302 2.085257e-02 0.025270758 ## 18: INVmissing 0.0109872677 1.660407e-02 0.010830325 ## 19: DEP 0.0096230438 6.018697e-03 0.028880866 ## 20: LORES 0.0079008679 3.831710e-03 0.025270758 ## 21: IRA1 0.0071695259 9.832480e-03 0.010830325 ## 22: INV1 0.0059168789 3.803924e-02 0.007220217 ## 23: BRANCHB16 0.0057965132 4.078194e-03 0.007220217 ## 24: BRANCHB14 0.0049738278 2.520413e-02 0.010830325 ## 25: CCPURC1 0.0048375451 1.807850e-02 0.007220217 ## 26: CC1 0.0044118913 1.871936e-02 0.007220217 ## 27: POSAMT 0.0042137344 8.918248e-04 0.010830325 ## 28: BRANCHB4 0.0041661842 2.357286e-03 0.010830325 ## 29: NSFAMT 0.0034087969 8.250500e-03 0.007220217 ## 30: MM1 0.0031540328 6.762631e-03 0.003610108 ## 31: POS 0.0027208141 8.201203e-04 0.010830325 ## 32: BRANCHB13 0.0024876723 9.836962e-03 0.003610108 ## 33: SDB1 0.0024214291 9.142324e-04 0.007220217 ## 34: BRANCHB10 0.0019911403 2.541028e-03 0.007220217 ## 35: BRANCHB9 0.0018719695 4.123009e-04 0.007220217 ## 36: BRANCHB6 0.0018585768 3.719671e-04 0.003610108 ## 37: BRANCHB15 0.0017458537 2.007726e-03 0.003610108 ## 38: BRANCHB12 0.0017102730 7.260081e-04 0.003610108 ## 39: PHONE 0.0016273237 3.181887e-04 0.003610108 ## 40: INVBAL 0.0014376920 3.630041e-04 0.003610108 ## 41: BRANCHB5 0.0012765723 1.792613e-04 0.003610108 ## 42: BRANCHB8 0.0009787161 2.285581e-04 0.003610108 ## 43: INCOME_flag1 0.0008286174 7.170451e-05 0.003610108 ## 44: MMCRED1 0.0006653432 6.274144e-05 0.003610108 ## 45: ACCTAGE_flag1 0.0006350179 6.722297e-05 0.003610108 ## 46: INAREA1 0.0005265657 9.859370e-05 0.003610108 ## Feature Gain Cover Frequency "],["model-evaluation.html", "Chapter 3 Model Evaluation", " Chapter 3 Model Evaluation "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
